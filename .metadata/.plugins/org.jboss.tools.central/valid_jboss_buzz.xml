<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>DevSecOps: Why you should care and how to get started</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/27/devsecops-why-you-should-care-and-how-get-started" /><author><name>Katrina Novakovic, Chris Jenkins</name></author><id>58940c33-3405-4639-a12d-77f71dc4e4e0</id><updated>2022-01-27T07:00:00Z</updated><published>2022-01-27T07:00:00Z</published><summary type="html">&lt;p&gt;The increasing popularity of &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; software development methodologies has led to shorter and more agile life cycles, in which software is released and deployed in minutes or hours rather than the days, weeks, or even months required under traditional practices. However, many development teams still experience delays in getting releases into production due to the &lt;a href="https://developers.redhat.com/topics/secure-coding"&gt;security&lt;/a&gt; considerations that are traditionally brought to bear at the end of the life cycle. To address this, organizations are more and more frequently adopting a &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; approach.&lt;/p&gt; &lt;h2&gt;What is DevSecOps?&lt;/h2&gt; &lt;p&gt;DevSecOps includes security in DevOps practices by embedding (or &lt;em&gt;left-shifting&lt;/em&gt;) security into applications early and continuously through a rapid, iterative, and automated software development life cycle (SDLC). The main goal is to produce more secure code more quickly. DevSecOps doesn't aim to turn developers into security experts, but rather educate them in best practices that promote more secure development processes.&lt;/p&gt; &lt;p&gt;In practical terms, DevSecOps:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Promotes secure coding standards and provides automated and repeatable testing.&lt;/li&gt; &lt;li aria-level="1"&gt;Uses static and dynamic code analysis to quickly find and remediate weaknesses and vulnerabilities in code.&lt;/li&gt; &lt;li aria-level="1"&gt;Applies secure configurations to provide a secure and compliant development environment.&lt;/li&gt; &lt;li aria-level="1"&gt;Continuously monitors the production environment for security threats and provides visible governance metrics for both security teams and developers.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why should developers care about DevSecOps?&lt;/h2&gt; &lt;p&gt;Organizations that foster a DevSecOps culture can become more agile and respond more quickly to change and innovation, while still meeting regulatory and organizational security goals. Development teams can roll out applications more quickly—without sacrificing security, while still meeting compliance standards. Automated compliance processes can lower costs, and historical data can be made visible and tracked to analyze trends and quickly identify potential vulnerabilities and exposures.&lt;/p&gt; &lt;p&gt;DevSecOps can decrease the potential risk for your organization and your customers by implementing a few simple strategies:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Writing secure code and sharing best practice guides across development teams. &lt;/strong&gt;This creates a more collaborative environment where everyone is responsible for security risk.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Identifying and fixing vulnerabilities earlier in the life cycle. &lt;/strong&gt;Typical security testing takes place near the end of the development cycle, such as during user acceptance testing or in staging or pre-production environments. Developers end up patching or rewriting code at this late stage when security flaws are discovered. That's counterproductive and increases time to production.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Implementing an integrated development environment (IDE) security plugin. &lt;/strong&gt;This can help developers check for security flaws while coding. There are many options, with two examples being the &lt;a href="https://owasp.org/www-community/Source_Code_Analysis_Tools"&gt;OWASP SAST plugin&lt;/a&gt; and the &lt;a href="https://snyk.io/ide-plugins/"&gt;Snyk IDE Plugin&lt;/a&gt;. Using the same IDE across development teams will also lower onboarding time and increase collaboration.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;How can you get started with DevSecOps?&lt;/h2&gt; &lt;p&gt;A shift to a DevSecOps philosophy will not happen overnight and will require buy-in at all levels of your organization. Planning and resources are required to begin the journey. A good place to start would be to identify developers who are already aware of the concepts; they can then become ambassadors to help enable other teams.&lt;/p&gt; &lt;p&gt;If your development teams want to implement DevSecOps for their projects successfully, the following suggestions can help overcome common challenges and get them started.&lt;/p&gt; &lt;h3&gt;Mindset and culture change&lt;/h3&gt; &lt;p&gt;Consider what a DevSecOps collaborative culture looks like for your development teams. How willing are team members to share the responsibility for security throughout the DevOps value chain? What is your &lt;em&gt;security debt&lt;/em&gt;—in other words, how many vulnerabilities have made it into production because developers have chosen not to fix them?&lt;/p&gt; &lt;p&gt;You'll want to identify security priorities, responsibilities, and communication paths for team members throughout the development life cycle. DevSecOps isn't just about providing tools; you'll also want to change people's perception of security and create more security-aware cross-functional teams. This fosters a culture where security is built in by default rather than bolted on at the end of a project.&lt;/p&gt; &lt;h3&gt;Learning and training&lt;/h3&gt; &lt;p&gt;Consider the additional security-related skills that developers and other team members need to acquire so that they can independently resolve security-related bugs. Formal in-house and external training can raise awareness and allow more experienced developers to mentor others within your organization. These mentors could then run short "Lunch and Learn" sessions with other developers to promote usage and understanding of DevSecOps practices within other development teams.&lt;/p&gt; &lt;p&gt;The following Red Hat courses and training can assist you:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://www.redhat.com/en/services/training/red-hat-devops-pipelines-and-processes-cicd-jenkins-do401?f%5B0%5D=hybrid_type%3ATraining"&gt;Red Hat DevOps Pipelines and Processes: CI/CD with Jenkins&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://www.redhat.com/en/services/certification/rhcs-security-linux?f%5B0%5D=hybrid_type%3ATraining"&gt;Red Hat Certified Specialist in Security: Linux&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Various articles in the &lt;a href="https://developers.redhat.com/learn"&gt;Red Hat Developer training program&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Integrate security into CI/CD pipelines&lt;/h3&gt; &lt;p&gt;A good way to start with DevSecOps is to create an initial team to evangelize its benefits. Start small so as not to be overwhelmed; for instance, the team could start with a small project that will enable them to hone their skills and create "ways of working" frameworks for other teams. The team should include members from the development, security, and infrastructure groups, as you'll need input from all these areas to plan the move to DevSecOps. Look at implementing a few essential security checks into the SDLC as a proof of concept, but remember to keep it simple at the beginning.&lt;/p&gt; &lt;p&gt;Tools such as &lt;a href="https://www.jenkins.io/"&gt;Jenkins&lt;/a&gt;, &lt;a href="https://circleci.com/"&gt;CircleCI&lt;/a&gt;, and &lt;a href="https://www.atlassian.com/software/bamboo"&gt;Bamboo&lt;/a&gt; will help automate the parts of software development related to building, testing, and deployment, and should include security checks in the process. If you already have &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous delivery (CI/CD)&lt;/a&gt; tools and processes, it should be quite straightforward to add security checks into the mix.&lt;/p&gt; &lt;p&gt;Examples of security checks include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Enforcing &lt;a href="https://developers.redhat.com/topics/secure-coding"&gt;secure coding&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/defensive-coding-guide"&gt;defensive coding&lt;/a&gt; standards.&lt;/li&gt; &lt;li aria-level="1"&gt;Looking for &lt;a href="https://developers.redhat.com/blog/2020/08/28/vulnerability-analysis-with-red-hat-codeready-dependency-analytics-and-snyk"&gt;source code vulnerabilities&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://cloud.redhat.com/blog/container-image-security-beyond-vulnerability-scanning"&gt;Scanning external libraries&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Using a &lt;a href="https://www.redhat.com/en/resources/ve-trusted-software-supply-chain-brief"&gt;trusted software supply chain&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Image vulnerability scanning (consider using a product such as &lt;a href="https://docs.openshift.com/acs/3.67/welcome/index.html"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;Identifying compromised credentials (e.g., passwords, secrets, certificates, API keys, and so on).&lt;/li&gt; &lt;li aria-level="1"&gt;Static application system testing (SAST), which analyzes development source code for security vulnerabilities and quality issues such as SQL injections or cross-site scripting.&lt;/li&gt; &lt;li aria-level="1"&gt;Dynamic application security testing (DAST), which analyzes running applications for execution vulnerabilities. This includes black box testing techniques such as fuzz testing.&lt;/li&gt; &lt;li aria-level="1"&gt;Active and passive penetration testing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The results of these tests should be fed into the CI/CD tool, along with the decisions made on the next steps. It's critical to provide feedback to developers at this stage so that security is not seen as a blocker to deployment.&lt;/p&gt; &lt;p&gt;From here, you'll be able to create common, sharable &lt;a href="https://www.redhat.com/en/events/webinar/automating-security-in-devops-environments"&gt;automated pipelines&lt;/a&gt; that include security checks into your application development processes. This approach will ensure that security and consistency are built into your applications from the very beginning. Security doesn't stop after deployment; continuous monitoring and alerting are required during the complete life cycle of an application.&lt;/p&gt; &lt;h2&gt;How can DevSecOps help with regulatory compliance?&lt;/h2&gt; &lt;p&gt;If your application manages payments, handles sensitive customer or patient data, or operates in a regulated market, then there are industry and regulatory standards that you need to meet and monitor. Some organizations may also require that you complete proof-of-compliance or authorization-to-operate documents before you can deploy applications into production environments.&lt;/p&gt; &lt;p&gt;Compliance goes beyond pure code and can also require additional artifacts, including:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Up-to-date documentation that describes data flow, mitigation against accidental data egress, and access controls. Documentation can also include instructions on the secure use of the application.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Following coding naming conventions and structured programming conventions will provide nearly self-documenting code, which can minimize the effort required to support and maintain that code.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Proposed patching processes for the application, with SLAs relevant to the associated risk categories.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Using the most up-to-date &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Images&lt;/a&gt; provides a higher level of assurance when it comes to vulnerabilities and patching.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Update and release controls to demonstrate and confirm security checks before deployment.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Using a &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; process can implement change control efficiently and effectively, with metrics that show where an application is in the SDLC.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Documentation on the complete supply chain, including a software bill of materials that shows what components and vulnerabilities are in the software and associated third-party libraries.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Check out the &lt;a href="https://www.linuxfoundation.org/webinars/generating-software-bill-of-materials/"&gt;Generating software bill of materials&lt;/a&gt; video produced by the Linux Foundation.&lt;/p&gt; &lt;p&gt;Weaving automated security compliance audits and controls into your SDLC will reduce the time developers will need to meet compliance requirements, allowing them to concentrate on developing software while adhering to all security policies.&lt;/p&gt; &lt;h2&gt;DevSecOps: Security + agility&lt;/h2&gt; &lt;p&gt;DevSecOps is all about automating and integrating security within all phases of the software development life cycle to produce more secure code more quickly and easily. Getting started requires that you change your mindset and organizational culture to collaborate and share responsibility for producing secure and compliant code, using tools and processes to implement security checks into CI/CD pipelines, and implementing automated security compliance audits and controls to comply with regulations. There is much more to DevSecOps, and you can explore it further as you build upon the foundation of these initial recommendations.&lt;/p&gt; &lt;p&gt;Here are a few resources to continue your DevSecOps journey:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.redhat.com/en/partners/devsecops"&gt;Modernize your life cycle and reduce risk with DevSecOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/resources/deploy-comprehensive-devsecops-solution-overview"&gt;How to deploy a comprehensive DevSecOps solution&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/resources/modernize-application-life-cycles-with-devsecops-e-book"&gt;Modernize and secure your application life cycles with DevSecOps&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can also explore tutorials, videos, and more on Red Hat Developer's &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; topic pages.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/27/devsecops-why-you-should-care-and-how-get-started" title="DevSecOps: Why you should care and how to get started"&gt;DevSecOps: Why you should care and how to get started&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Katrina Novakovic, Chris Jenkins</dc:creator><dc:date>2022-01-27T07:00:00Z</dc:date></entry><entry><title>Build a bootable JAR for cloud-ready microservices</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/26/build-bootable-jar-cloud-ready-microservices" /><author><name>Mauro Vocale</name></author><id>4db2da6e-c28b-4df6-a2ee-2deb1347b874</id><updated>2022-01-26T07:00:00Z</updated><published>2022-01-26T07:00:00Z</published><summary type="html">&lt;p&gt;For &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; running in &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; environments, &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; developers tend to want a self-contained image that incorporates the complete runtime environment needed to run an application. At the same time, developers want a minimally sized image for both efficiency and security. A bootable JAR can meet these requirements. This article describes how to create a bootable JAR using &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) and &lt;a href="https://jakarta.ee/"&gt;Jakarta EE&lt;/a&gt; and incorporate useful extensions, particularly a &lt;a href="https://www.postgresql.org"&gt;PostgreSQL&lt;/a&gt; database and &lt;a href="https://microprofile.io/"&gt;MicroProfile&lt;/a&gt; capabilities.&lt;/p&gt; &lt;h2&gt;About the example application&lt;/h2&gt; &lt;p&gt;For this demonstration, I have updated the application used in my &lt;a href="https://developers.redhat.com/articles/2021/06/25/making-java-programs-cloud-ready-part-1-incremental-approach-using-jakarta-ee"&gt;previous series of articles&lt;/a&gt;. Like that series, this article runs the application on an instance of &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;, an open source Platform-as-a-Service (PaaS) based on &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;. One of my priorities for all of these demonstrations is to optimize the consumption of resources that are the main factors in the costs charged by a PaaS vendor: image size, memory use, and CPU use.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://docs.wildfly.org/bootablejar/"&gt;bootable JAR&lt;/a&gt; is a feature of &lt;a href="https://www.wildfly.org/"&gt;WildFly&lt;/a&gt;, a lightweight runtime for building Java applications. We'll use JBoss EAP, a fully supported enterprise Java platform based on WildFly. We'll provision custom layers that expand the application's capabilities using &lt;a href="https://docs.wildfly.org/galleon/"&gt;Galleon feature-packs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The source code for this application is available on my &lt;a href="https://github.com/mvocale/JBoss_EAP_cloud_ready"&gt;GitHub repository&lt;/a&gt;. To track the application's evolution, I created a new tag named &lt;code&gt;Galleon_Runtime_EAP_XP_bootable_jar_version&lt;/code&gt; for the version used in this article. You can use the tags assigned to repository branches to analyze the configuration information through various iterations.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You will need the following software to execute the example application:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A Red Hat OpenShift installation (I used &lt;a href="https://developers.redhat.com/products/openshift/download"&gt;OpenShift 4.8&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/eap/download"&gt;JBoss EAP 7.4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/eap/download"&gt;JBoss EAP XP 3.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://maven.apache.org/download.cgi"&gt;Apache Maven 3.8.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/openjdk/overview"&gt;OpenJDK 11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://git-scm.com/"&gt;Git 2.31.1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you prefer, you could use &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt; to run an OpenShift instance on your local system.&lt;/p&gt; &lt;h2&gt;Get the source code&lt;/h2&gt; &lt;p&gt;To install the &lt;a href="https://github.com/mvocale/JBoss_EAP_cloud_ready"&gt;source code from my GitHub repository&lt;/a&gt;, open a terminal, select a folder, and clone the repository using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/mvocale/JBoss_EAP_cloud_ready.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, change into the directory for the project. Check out the &lt;code&gt;Galleon_Runtime_EAP_XP_bootable_jar_version&lt;/code&gt; version, where I stored the code used to implement the bootable JAR running mode, using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git checkout tags/Galleon_Runtime_EAP_XP_bootable_jar_version&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt; subdirectory contains the main application, copied from an earlier version of the same application. The source code uses the Jakarta EE 8 and MicroProfile 3 specifications on top of JBoss EAP XP 3 in bootable JAR mode, employing Galleon to install only the required subsystems. The final container image was improved using the runtime version of OpenJDK 11.&lt;/p&gt; &lt;p&gt;The application needs a database to store information, so I chose PostgreSQL. The &lt;code&gt;postgresql-database-layer&lt;/code&gt; subdirectory of the repository loads and configures the database. I could have simply used a feature-pack from the &lt;a href="https://github.com/jbossas/eap-datasources-galleon-pack"&gt;wildfly-datasources-galleon-pack&lt;/a&gt; project, which supports PostgreSQL as well as Microsoft SQL Server and Oracle. But one goal of this article is to show how to customize a JBoss EAP subsystem with features not supported by default, or how to change the behavior of a feature through a custom layer.&lt;/p&gt; &lt;p&gt;A shell script named &lt;code&gt;deploy-openshift.sh&lt;/code&gt; contains all the instructions needed to install all the implemented components on top of OpenShift. If you don't want to perform every single step described, you can establish a connection to CodeReady Containers or an OpenShift remote cluster and run the script there.&lt;/p&gt; &lt;h2&gt;Set up your environment&lt;/h2&gt; &lt;p&gt;Now it's time to connect to OpenShift to deploy all the components needed by our application. If you are using CodeReady Containers, start it and log in as a developer using the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ crc start $ oc login -u developer -p developer https://api.crc.testing:6443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Otherwise, log into your OpenShift environment as follows, substituting your own appropriate values for &lt;code&gt;$token&lt;/code&gt; and &lt;code&gt;$server_url&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login --token=$token --server=$server_url&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the project that will host the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project redhat-jboss-eap-cloud-ready-demo --display-name="Red Hat JBoss EAP Cloud Ready Demo"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create and configure the PostgreSQL database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Import image related to Postgresql Database $ oc import-image rhel8/postgresql-13:1-21 --from=registry.redhat.io/rhel8/postgresql-13:1-21 --confirm # Create the Postgresql Database Application $ oc new-app -e POSTGRESQL_USER=mauro \ -e POSTGRESQL_PASSWORD=secret \ -e POSTGRESQL_DATABASE=weather postgresql-13:1-21 \ --name=weather-postgresql &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the PostgreSQL icon to the database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc patch dc weather-postgresql --patch '{"metadata": { "labels": { "app.openshift.io/runtime": "postgresql" } } }'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, deploy a set of actors that will help you get the benefits of MicroProfile specifications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To install these projects, switch to the &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt; directory that hosts the application source code and run the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Import Jaeger image from catalog $ oc import-image distributed-tracing/jaeger-all-in-one-rhel8:1.24.1-1 --from=registry.redhat.io/distributed-tracing/jaeger-all-in-one-rhel8:1.24.1-1 --confirm # Create the Jaeger application $ oc new-app -i jaeger-all-in-one-rhel8:1.24.1-1 # Expose the route in order to make the Jaeger application available outside of OpenShift $ oc expose svc jaeger-all-in-one-rhel8 --port=16686 # Create the Prometheus environment used to collect the values provided by MicroProfile Metrics specifications. Import the Prometheus image from catalog $ oc import-image openshift4/ose-prometheus:v4.8.0-202110011559.p0.git.f3beb88.assembly.stream --from=registry.redhat.io/openshift4/ose-prometheus:v4.8.0-202110011559.p0.git.f3beb88.assembly.stream --confirm # Create the config map with the Prometheus configurations $ oc create configmap prometheus --from-file=k8s/prometheus.yml ### Create the Prometheus application $ oc create -f k8s/ose-prometheus.yaml # Create the Grafana environment used to collect the values provided by MicroProfile Metrics specifications. Import Grafana image from catalog $ oc import-image openshift4/ose-grafana:v4.8.0-202110011559.p0.git.b987e4b.assembly.stream --from=registry.redhat.io/openshift4/ose-grafana:v4.8.0-202110011559.p0.git.b987e4b.assembly.stream --confirm # Create the config map with the Grafana configurations $ oc create configmap grafana --from-file=k8s/datasource-prometheus.yaml --from-file=k8s/grafana-dashboard.yaml --from-file=k8s/jboss_eap_grafana_dashboard.json # Create the Grafana application $ oc create -f k8s/ose-grafana.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Build a custom layer for JBoss EAP XP&lt;/h2&gt; &lt;p&gt;The version of the application created in my previous articles used Source-to-Image (S2I) to define and configure the JDBC driver and data source used by the application (see &lt;a href="https://github.com/mvocale/JBoss_EAP_cloud_ready/blob/Galleon_Runtime_version/weather-app-eap-cloud-ready/extensions/install.sh"&gt;the installation shell script&lt;/a&gt; for details). I had a folder named &lt;code&gt;extensions&lt;/code&gt; where I defined:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;module.xml&lt;/code&gt; file and JAR file used by the JBoss EAP module&lt;/li&gt; &lt;li&gt;The &lt;code&gt;drivers.env&lt;/code&gt; file, where I specified the driver properties&lt;/li&gt; &lt;li&gt;The &lt;code&gt;install.sh&lt;/code&gt; file to instruct the S2I build where to find the resources needed to configure the JBoss EAP driver and data source&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;code&gt;postgresql-database-layer&lt;/code&gt; subproject builds all the resources needed to provision and set up the JDBC driver and the data source subsystem. Let's change into this subproject:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd postgresql-database-layer&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This project creates a layer to provision and configure the driver module and the data source subsystem that manages the JDBC connection to the PostgreSQL database. Under the &lt;code&gt;src/main/resources&lt;/code&gt; directory are two subdirectories:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;layers/standalone&lt;/code&gt;: Contains the files needed to configure the driver (&lt;code&gt;postgresql-driver&lt;/code&gt;) and the data source (&lt;code&gt;postgresql-datasource&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;code&gt;module/org/postgresql/main&lt;/code&gt;: Contains the file needed to manage the JBoss module used to interact with the PostgreSQL database.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the &lt;code&gt;postgresql-driver&lt;/code&gt; directory lies a &lt;code&gt;layer-spec.xml&lt;/code&gt; file that contains the parameters used by the Galleon framework to provision and configure the JDBC driver:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0" ?&gt; &lt;layer-spec xmlns="urn:jboss:galleon:layer-spec:1.0" name="postgresql-driver"&gt; &lt;feature spec="subsystem.datasources"&gt; &lt;feature spec="subsystem.datasources.jdbc-driver"&gt; &lt;param name="driver-name" value="postgresql"/&gt; &lt;param name="jdbc-driver" value="postgresql"/&gt; &lt;param name="driver-xa-datasource-class-name" value="org.postgresql.xa.PGXADataSource"/&gt; &lt;param name="driver-module-name" value="org.postgresql"/&gt; &lt;/feature&gt; &lt;/feature&gt; &lt;packages&gt; &lt;package name="org.postgresql"/&gt; &lt;/packages&gt; &lt;/layer-spec&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Under the &lt;code&gt;postgresql-datasource&lt;/code&gt; directory lies a &lt;code&gt;layer-spec.xml&lt;/code&gt; file that contains the parameters used by the Galleon framework to provision and configure the data source:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0"?&gt; &lt;layer-spec xmlns="urn:jboss:galleon:layer-spec:1.0" name="postgresql-datasource"&gt; &lt;dependencies&gt; &lt;layer name="postgresql-driver" /&gt; &lt;/dependencies&gt; &lt;feature spec="subsystem.datasources.data-source"&gt; &lt;param name="use-ccm" value="true" /&gt; &lt;param name="data-source" value="WeatherDS" /&gt; &lt;param name="enabled" value="true" /&gt; &lt;param name="use-java-context" value="true" /&gt; &lt;param name="jndi-name" value="${env.DB_JNDI}" /&gt; &lt;param name="connection-url" value="jdbc:postgresql://${env.WEATHER_POSTGRESQL_SERVICE_HOST}:${env.WEATHER_POSTGRESQL_SERVICE_PORT}/${env.DB_DATABASE}" /&gt; &lt;param name="driver-name" value="postgresql" /&gt; &lt;param name="user-name" value="${env.DB_USERNAME}" /&gt; &lt;param name="password" value="${env.DB_PASSWORD}" /&gt; &lt;param name="validate-on-match" value="false"/&gt; &lt;param name="valid-connection-checker-class-name" value="org.jboss.jca.adapters.jdbc.extensions.postgres.PostgreSQLValidConnectionChecker"/&gt; &lt;param name="exception-sorter-class-name" value="org.jboss.jca.adapters.jdbc.extensions.postgres.PostgreSQLExceptionSorter"/&gt; &lt;param name="background-validation" value="true" /&gt; &lt;param name="background-validation-millis" value="60000" /&gt; &lt;param name="flush-strategy" value="IdleConnections" /&gt; &lt;param name="statistics-enabled" value="${wildfly.datasources.statistics-enabled:${wildfly.statistics-enabled:true}}" /&gt; &lt;/feature&gt; &lt;/layer-spec&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, under the &lt;code&gt;src/main/resources/modules/org/postgresql/main&lt;/code&gt; directory is a &lt;code&gt;module.xml&lt;/code&gt; file that configures the JBoss module:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;module xmlns="urn:jboss:module:1.0" name="org.postgresql"&gt; &lt;resources&gt; &lt;artifact name="${org.postgresql:postgresql}"/&gt; &lt;/resources&gt; &lt;dependencies&gt; &lt;module name="javax.api"/&gt; &lt;module name="javax.transaction.api"/&gt; &lt;/dependencies&gt; &lt;/module&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;pom.xml&lt;/code&gt; file in the &lt;code&gt;postgresql-database-layer&lt;/code&gt; directory builds the custom feature-pack using the Maven Galleon plug-in:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;properties&gt; &lt;version.org.postgresql&gt;42.2.18.redhat-00001&lt;/version.org.postgresql&gt; &lt;version.wildfly.galleon.pack&gt;3.0.0.GA-redhat-00005&lt;/version.wildfly.galleon.pack&gt; &lt;version.wildfly.galleon.maven.plugin&gt;5.2.4.Final&lt;/version.wildfly.galleon.maven.plugin&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Import the Postgresql JDBC driver --&gt; &lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;${version.org.postgresql}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.eap&lt;/groupId&gt; &lt;artifactId&gt;wildfly-galleon-pack&lt;/artifactId&gt; &lt;version&gt;${version.wildfly.galleon.pack}&lt;/version&gt; &lt;type&gt;zip&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.wildfly.galleon-plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-galleon-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.wildfly.galleon.maven.plugin}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;wildfly-datasources-galleon-pack-build&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build-user-feature-pack&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;configuration&gt; &lt;translate-to-fpl&gt;true&lt;/translate-to-fpl&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;build-user-feature-pack&lt;/code&gt; goal in the Galleon Maven plug-in builds the custom layers.&lt;/p&gt; &lt;h2&gt;Build a JBoss EAP XP 3 bootable JAR&lt;/h2&gt; &lt;p&gt;Unlike the example in my previous &lt;a href="https://developers.redhat.com/articles/2021/06/25/making-java-programs-cloud-ready-part-1-incremental-approach-using-jakarta-ee"&gt;articles&lt;/a&gt;, this example does not put the application into a JBoss EAP XP container image as a deployment artifact. Instead, we create a bootable JAR containing a server, a packaged application, and the runtime required to launch the application. The source code remains the same.&lt;/p&gt; &lt;p&gt;Using a bootable JAR for deployment changes the following aspects of the previous example:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;How we provision and configure database connectivity: We use the feature-pack implemented through the &lt;code&gt;postgresql-database-layer&lt;/code&gt; subproject.&lt;/li&gt; &lt;li&gt;How we build the application: We use the &lt;code&gt;wildfly-jar-maven-plugin&lt;/code&gt; Maven plug-in to create a bootable JAR.&lt;/li&gt; &lt;li&gt;How we build the application image: Instead of a JBoss EAP XP image, we use an OpenJDK image at build time and runtime, since we created a bootable JAR.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;code&gt;wildfly-jar-maven-plugin&lt;/code&gt; plug-in is invoked in the &lt;code&gt;pom.xml&lt;/code&gt; file in the &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt; subproject::&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-jar-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${bootable.jar.maven.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;feature-packs&gt; &lt;feature-pack&gt; &lt;location&gt;org.jboss.eap:wildfly-galleon-pack:${version.wildfly.galleon.pack}&lt;/location&gt; &lt;/feature-pack&gt; &lt;feature-pack&gt; &lt;groupId&gt;com.redhat.examples&lt;/groupId&gt; &lt;artifactId&gt;postgresql-layer&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/feature-pack&gt; &lt;/feature-packs&gt; &lt;layers&gt; &lt;layer&gt;jaxrs-server&lt;/layer&gt; &lt;layer&gt;microprofile-platform&lt;/layer&gt; &lt;layer&gt;postgresql-datasource&lt;/layer&gt; &lt;/layers&gt; &lt;cloud /&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;package&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's analyze the core elements of the Maven plug-in. The first important element is the &lt;code&gt;&lt;feature-pack&gt;&lt;/code&gt; tag. The JBoss EAP JAR Maven plug-in uses Galleon's trimming capability to reduce the size and memory footprint of the server. Thus, you can configure the server according to your requirements, including only the Galleon layers that provide the capabilities you need.&lt;/p&gt; &lt;p&gt;To specify the layers we want, I use two feature-packs:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;wildfly-galleon-plugin&lt;/code&gt; specifies the layers provided by JBoss EAP XP.&lt;/li&gt; &lt;li&gt;&lt;code&gt;postgresql-layer&lt;/code&gt; is the custom layer that builds the data source subsystem described in the previous section. To refer to this layer, I use the Maven GAV coordinates (group ID, artifact ID, and version) specified in the project's &lt;code&gt;pom.xml&lt;/code&gt; file.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Those two feature-packs are used by the Galleon framework to build JBoss EAP XP with only the requested subsystems. The &lt;code&gt;&lt;layers&gt;&lt;/code&gt; tag specifies the required layers:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;jaxrs-server&lt;/code&gt;: This layer contains the subsystem needed to implement the Servlet, JAX-RS, JPA, and CDI Jakarta EE specifications.&lt;/li&gt; &lt;li&gt;&lt;code&gt;microprofile-platform&lt;/code&gt;: This decorator layer adds the MicroProfile capabilities to the provisioned server.&lt;/li&gt; &lt;li&gt;&lt;code&gt;postgresql-datasource&lt;/code&gt;: This is my custom decorator layer that I created to manage the data source subsystem and the JDBC driver.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;code&gt;&lt;cloud/&gt;&lt;/code&gt; tag appears in the &lt;code&gt;&lt;configuration&gt;&lt;/code&gt; element of the plug-in configuration in the &lt;code&gt;pom.xml&lt;/code&gt; file so that the JBoss EAP Maven JAR plug-in can recognize that you chose the OpenShift platform.&lt;/p&gt; &lt;p&gt;Run Maven locally to create the &lt;code&gt;weather-app-cloud-ready-1.0-bootable.jar&lt;/code&gt; JAR file containing all you need to deploy the application into JBoss EAP XP inside OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Update the buildConfig&lt;/h2&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2021/07/02/making-java-programs-cloud-ready-part-4-optimize-runtime-environment#runtime_optimization_with_jboss_eap__jboss_eap_xp__and_galleon"&gt;Part 4&lt;/a&gt; of my previous series, I showed how to create a chained build to produce a runtime image that contained only the resources needed to execute the application. Using the bootable JAR mode with JBoss EAP XP, we can follow the same approach. This time, we import the OpenJDK container images:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Import image related to OpenJDK 11 $ oc import-image ubi8/openjdk-11:1.10-1 --from=registry.access.redhat.com/ubi8/openjdk-11:1.10-1 --confirm # Import image related to OpenJDK 11 - Runtime $ oc import-image ubi8/openjdk-11-runtime:1.10-1 --from=registry.access.redhat.com/ubi8/openjdk-11-runtime:1.10-1 --confirm&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have updated the &lt;code&gt;buildConfig.yaml&lt;/code&gt; file under the &lt;code&gt;k8s&lt;/code&gt; directory. That file now defines a chained build with two &lt;code&gt;buildConfig&lt;/code&gt; objects: &lt;code&gt;weather-app-eap-cloud-ready-build-artifacts&lt;/code&gt; and &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt;. The first object is a base platform image for building and running plain Java 11 applications, such as a fat JAR and a flat classpath. The container image contains S2I integration scripts for deployment on OpenShift.&lt;/p&gt; &lt;p&gt;The second object is a lean, runtime-only container designed to be a base for deploying prebuilt applications. The container does not contain the Java compiler, the JDK tools, or Maven. Here, I put my fat JAR containing the JBoss EAP XP server and the application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kind: ImageStream apiVersion: image.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready-build-artifacts labels: application: weather-app-eap-cloud-ready-build-artifacts --- kind: ImageStream apiVersion: image.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready labels: application: weather-app-eap-cloud-ready --- kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready-build-artifacts namespace: redhat-jboss-eap-cloud-ready-demo labels: build: weather-app-eap-cloud-ready-build-artifacts spec: output: to: kind: ImageStreamTag name: 'weather-app-eap-cloud-ready-build-artifacts:latest' resources: {} strategy: type: Source sourceStrategy: env: - name: ARTIFACT_DIR value: weather-app-eap-cloud-ready/target from: kind: ImageStreamTag namespace: redhat-jboss-eap-cloud-ready-demo name: 'openjdk-11:1.10-1' source: type: Binary binary: {} --- kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: labels: application: weather-app-eap-cloud-ready name: weather-app-eap-cloud-ready spec: output: to: kind: ImageStreamTag name: weather-app-eap-cloud-ready:latest source: images: - from: kind: ImageStreamTag name: weather-app-eap-cloud-ready-build-artifacts:latest paths: - sourcePath: /deployments destinationDir: ./deployments dockerfile: |- FROM openjdk-11:1.10-1 COPY deployments / CMD java -jar /deployments/weather-app-cloud-ready-1.0-bootable.jar strategy: dockerStrategy: imageOptimizationPolicy: SkipLayers from: kind: ImageStreamTag name: openjdk-11-runtime:1.10-1 namespace: redhat-jboss-eap-cloud-ready-demo type: Docker triggers: - imageChange: {} type: ImageChange&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the application&lt;/h2&gt; &lt;p&gt;Now it's time to create the &lt;code&gt;ImageStream&lt;/code&gt; instances and the chained &lt;code&gt;buildConfig&lt;/code&gt; to build and deploy the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Move to the project directory $ cd weather-app-eap-cloud-ready # Create the ImageStreams and the chained builds config to make the runtime image with JBoss EAP XP 3 and the application $ oc create -f k8s/buildConfig.yaml # Move to the project root $ cd .. # Start the build of the application on OpenShift $ oc start-build weather-app-eap-cloud-ready-build-artifacts --from-dir=. --wait&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I suggest checking when the second build is finished by using this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get build weather-app-eap-cloud-ready-1 --watch&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the status moves from &lt;code&gt;Pending&lt;/code&gt; to &lt;code&gt;Complete&lt;/code&gt;, you can create the weather application for JBoss EAP XP 3 and configure all the needed resources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f k8s/weather-app-eap-cloud-ready.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can then test the application, using the steps described in the &lt;a href="https://developers.redhat.com/articles/2021/06/30/making-java-programs-cloud-ready-part-3-integrate-microprofile-services#activate_the_microprofile_services"&gt;previous articles&lt;/a&gt;, to verify that it works. Although I revisited the way to build and deploy my application, the source code and the features remain the same. I can continue to use all the specifications provided by Jakarta EE and MicroProfile using the fat JAR mode, like the majority of Java cloud-friendly frameworks.&lt;/p&gt; &lt;h2&gt;Resource optimization&lt;/h2&gt; &lt;p&gt;Using the Galleon framework for a container runtime image reduces the use of expensive resources. In &lt;a href="https://developers.redhat.com/articles/2021/07/02/making-java-programs-cloud-ready-part-4-optimize-runtime-environment#reviewing_the_outcomes"&gt;Part 4&lt;/a&gt; of my previous series, I showed how you can save about 35% of the space in the memory image and about 28% of its memory footprint using this approach instead of the traditional container image with the full JBoss EAP XP framework and all the developer tools. I obtained the same result using the bootable JAR approach.&lt;/p&gt; &lt;p&gt;Figure 1 shows the size of the container image with OpenJDK and the bootable JAR (JBoss EAP XP plus the application).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/size.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/size.png?itok=e9WUw9M_" width="1374" height="426" alt="The size of the complete image is only 248.1 MiB." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The size of the complete image is only 248.1 MiB. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 2 shows the memory footprint.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/footprint.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/footprint.png?itok=5eK4KAAO" width="1440" height="91" alt="The memory footprint of running image is only 403.1 MiB." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The memory footprint of running image is only 403.1 MiB. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;As these figures show, I obtained results similar to the previous series without changing my code.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article shows how to modernize your application to run in the cloud while implementing the features provided by Jakarta EE and MicroProfile. I demonstrated an approach using a bootable JAR, like the majority of the Java frameworks aimed at cloud deployment, without needing to change any application code. You can start using this approach now or adopt it later.&lt;/p&gt; &lt;p&gt;The impact on your code is very minimal with this approach, so you can keep evolving your applications as you need to. Continuous improvement is key to the success of your architecture.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/26/build-bootable-jar-cloud-ready-microservices" title="Build a bootable JAR for cloud-ready microservices"&gt;Build a bootable JAR for cloud-ready microservices&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mauro Vocale</dc:creator><dc:date>2022-01-26T07:00:00Z</dc:date></entry><entry><title type="html">Calling Java from DMN</title><link rel="alternate" href="https://blog.kie.org/2022/01/dmn-calls-java.html" /><author><name>Donato Marrazzo</name></author><id>https://blog.kie.org/2022/01/dmn-calls-java.html</id><updated>2022-01-25T17:26:30Z</updated><content type="html">Decision Model and Notation (DMN) is designed to catch the crucial business logic which affects decisions. It’s not meant to be a replacement for a general purpose programming language. FEEL is a powerful expression, but it is on purpose designed to be an expression language and not an imperative language to avoid bringing in the DMN unnecessary complexities which should be addressed somewhere else. For example, the "french amortization method" is widely used in the financial industry, however it does not make a lot of sense to implement it in DMN. Why not? Because, it’s a fixed algorithm not subject to change, the implementation is already available somewhere else and more importantly business users don’t want to change that logic. Whereas, It’s likely that business users want to adjust the algorithm parameters based on the customer features: eventually, this part of the logic is eligible to be designed in a DMN. In short, at the design stage you have to decide what you need to implement in Java (or another language) and what you need to implement in DMN. The input data of a decision can be pre-processed by an application logic, the same applies to the output data which can be enriched or used by a complex algorithm. In some cases, complex data manipulation may be required within a decision, which is difficult to implement in FEEL but easy to write in Java. Inside DMN, you can call a static method of a Java class. The Function call can happen inside a Context or in Business Knowledge Model, in essence: 1. Click in the corner and switch from FEEL to JAVA Select Java 2. Define the Fully Qualified Name of the Java Class 3. Define the method signature. 4. Define the parameter list. Here the final outcome: Java function Pay attention: in Java a string is a class, so you have to type the fully qualified name: java.lang.String. In the following video, a step by step tutorial using Kogito DMN: REFERENCES * * * * The post appeared first on .</content><dc:creator>Donato Marrazzo</dc:creator></entry><entry><title type="html">How to configure an Elytron JAAS Security Realm</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-security/how-to-configure-an-elytron-jaas-security-realm/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-an-elytron-jaas-security-realm" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-security/how-to-configure-an-elytron-jaas-security-realm/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-an-elytron-jaas-security-realm</id><updated>2022-01-25T09:44:43Z</updated><content type="html">This article is a walk though the configuration of an Elytron JAAS security Realm on WildFly application server. We will shortly review how JAAS configuration works and then we will deploy an example application which leverages the JAAS Security Configuration file. JAAS in a nutshell The Java Authentication and Authorization Service (JAAS) is a security ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>The disadvantages of microservices</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/25/disadvantages-microservices" /><author><name>Bob Reselman</name></author><id>f99b01ca-aff1-45d0-8154-58079bd419ef</id><updated>2022-01-25T07:00:00Z</updated><published>2022-01-25T07:00:00Z</published><summary type="html">&lt;p class="Indent1"&gt;This article is the last part of a three-part series describing the design and implementation of microservices. The first part laid out &lt;a href="http://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices"&gt;5 design principles for microservices&lt;/a&gt;, and the second part showed &lt;a href="http://developers.redhat.com/articles/2022/01/19/monolith-microservices-how-applications-evolve"&gt;how monolithic applications tend to evolve into microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;Microservices&lt;/a&gt; have a lot to offer to today's application development. When implemented properly, they allow applications to change quickly without side effects. But they are not a panacea for all woes in software development. They do come with trade-offs. &lt;/p&gt; &lt;p&gt;In this article, we look at the potential drawbacks of microservices, and considerations you should think about before implementing them.&lt;/p&gt; &lt;h2&gt;More complexity in exchange for more flexibility&lt;/h2&gt; &lt;p&gt;A microservice-oriented architecture (MOA) incurs more complexity in order to get more flexibility. In a monolithic application, all the parts are in one box, so to speak. Therefore, you can declare dependencies in code and not worry too much about deployment issues. All the parts ship together at deployment time.&lt;/p&gt; &lt;p&gt;With an MOA, all the parts are all over the place on the network. Thus, you have to spend more time "wiring" the microservices together to get the overall application up and running. The wiring involves not only identifying the microservice on the network but also enabling authorized access to all the microservices that make up the MOA. This task includes providing credentials so the MOA can access the particular microservice and also fiddling with firewall and router settings at the physical level of the application's operation.&lt;/p&gt; &lt;p&gt;The bottom line is that microservices do indeed offer more flexibility in terms of maintenance and upgrade, but that increased flexibility comes at the price of increased complexity.&lt;/p&gt; &lt;h2&gt;More resources in exchange for greater independence&lt;/h2&gt; &lt;p&gt;MOAs require a lot of horsepower. Remember, each microservice in the MOA comes with its own runtime environment and data storage. Thus, in some cases, even the most scaled-down microservice can end up eating up as many resources as a single monolithic application. And that's only on the hardware end of things once the MOA is in production.&lt;/p&gt; &lt;p&gt;In terms of development, each microservice in an MOA requires its own set of developers, its own source code management system, its own testing process, and its own set of scripts to support automated deployment. This all adds up. But it's the price that comes with the independence that microservices provide.&lt;/p&gt; &lt;p&gt;In short, it's a trade-off. Many companies are willing to pay the premium that goes with an MOA in order to get the operational independence required for faster release cycles.&lt;/p&gt; &lt;h2&gt;More orchestration in exchange for reliable operation&lt;/h2&gt; &lt;p&gt;Implementing a microservices-oriented application is not a matter of creating a few independent services and deploying them to the network along with a configuration file that ties the services together. There's a lot more planning involved.&lt;/p&gt; &lt;p&gt;Fortunately, the complexities of creating, deploying, and supporting an MOA have revealed design patterns that have become well known over the years. In turn, these patterns have been used to create orchestration frameworks that are well suited to supporting MOAs. Probably the best known of these orchestration frameworks is &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Kubernetes makes it so that developers and system administrators can run &lt;a href="http://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; as services on a network within a data center. Kubernetes also provides the capability to wire these services together to create a microservices-oriented application. To quote Kubernetes evangelist Kelsey Hightower, "the data center is the computer."&lt;/p&gt; &lt;p&gt;Kubernetes is a transformational technology. It's also a complex technology that has a significant learning curve. But it can run very large applications with a high degree of reliability. Kubernetes has the wherewithal to guarantee that once you've configured an MOA, Kubernetes will keep it all going in the face of many dangers. It's not unusual for Kubernetes to run an MOA that has dozens, if not hundreds, of microservices.&lt;/p&gt; &lt;p&gt;Kubernetes is quite a technical achievement. Despite its complexity, you need this type of orchestration framework to run an MOA at scale. An MOA has a lot of moving parts, of which any one can fail at any time. Keeping it all up and running is beyond human capability. Hence, the trade-off is that to run a microservices-oriented application reliably, you need to incur the complexity of an orchestration framework.&lt;/p&gt; &lt;h2&gt;Putting it all together&lt;/h2&gt; &lt;p&gt;Microservice-oriented applications are a compelling approach to software development in today's world. The emergence of TCP/IP and the internet has created networking standards that make MOAs possible. Segmenting applications into discrete, independent microservices creates a high degree of flexibility that enables shorter release cycles. A well-constructed MOA gets better software into the hands of those who need it at faster rates.&lt;/p&gt; &lt;p&gt;But MOAs come with a price. Getting a number of microservices to work together reliably as a microservices-oriented application is a complex undertaking. The trade-offs can be significant, but so are the benefits. MOAs have a lot to offer companies that have applications that need to run at web scale and provide benefits to a large number of users.&lt;/p&gt; &lt;h2 id="learn_more_about_gitops_and_ci_cd_on_red_hat_developer-h2"&gt;Learn more about microservices on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Check out the following resources for more tips about building and deploying microservices:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Download chapters from the upcoming e-book &lt;a href="https://developers.redhat.com/e-books/kubernetes-native-microservices-quarkus-and-microprofile"&gt;&lt;em&gt;Kubernetes Native Microservices with Quarkus and MicroProfile&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices"&gt;5 design principles for microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/14/application-modernization-patterns-apache-kafka-debezium-and-kubernetes#"&gt;Application modernization patterns with Apache Kafka, Debezium, and Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/21/distributed-transaction-patterns-microservices-compared"&gt;Distributed transaction patterns for microservices compared&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/16/elegant-way-performance-test-microservices-kubernetes"&gt;An elegant way to performance test microservices on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/25/disadvantages-microservices" title="The disadvantages of microservices"&gt;The disadvantages of microservices&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-25T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Example HL7 and FHIR integration</title><link rel="alternate" href="http://www.schabell.org/2022/01/idaas-example-hl7-and-fhir-integration.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/01/idaas-example-hl7-and-fhir-integration.html</id><updated>2022-01-25T06:00:00Z</updated><content type="html">Part 4 - Example HL7 &amp;amp; FHIR integration In our  from this series we talked about the example iDaaS data architecture which outlines at a higher abstraction level the solution for any healthcare organisation. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture. It continued by laying out the process of how we approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Having completed our discussions on the example data integration architecture, we'll walk you through a more specific example. This scenario is showing a proven and scalable architecture for integrating HL7 and FHIR solutions into your healthcare organisation. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. IDAAS HL7 AND FHIR INTEGRATION In the previous article, details were given for a general overview of the data integration architecture for iDaaS. In this example the goal is not focus more on the collection of iDaaS Connect integration services and how they are specifically architected to integrate HL7 and FHIR systems in a healthcare organisation. With that in mind, let's starting on the left side of the diagram where the entry point for actors in this architecture can be found. They're representing the edge devices as well as the users submitting requests and receiving responses.  These users are representing patients, vendors, suppliers, and clinical personnel. The administrators is representing the clinical personnel tasked with managing the various healthcare processes. All requests enter through the API management element, used to secure and authenticate access to internal services and applications. The first collection of elements encountered is labeled as iDaaS Connect, where we find all the various integration services for specific communication channels as discussed in the previous article.  Specific to this example, the figure narrows the scope of iDaaS Connect to just those required for HL7 and FHIR integration scenarios. Two elements are shown and each one is a specific collection of microservices implementing their respective communication standards. The HL7 connect microservices element provides connectivity to systems and organisations that have adopted this specific healthcare standard. A FHIR connect microservices element does the same for that healthcare standard. Backing both of these elements is a message transformation microservices element. This is used to ensure messages are in the formats needed for the eventual backend events that need to take place when they come into the architecture, or translated back out to the right standard for the end point consumer.  After all of the standards integration and eventual message transformation activities, iDaaS Connect services register events (and receive event notification from) the iDaaS connect events. This is a central hub that ensures all events are registered, managed, and notifications are sent to the appropriate elements in the iDaaS architecture.  Events will often trigger elements of the iDaaS DREAM collection to take action, through the iDaaS event builder which captures business automation activities and the iDaaS intelligent data router. This data router is capable of managing where specific data needs to be sent, both inbound to sources and outbound to application or service destinations. It's assisted by the iDaaS connect data distribution element which ensures integration with all manner of data sources which might be in local or remote locations such as a public cloud. There are all manner of possible, and optional, external services that a healthcare organisation might integrate from third-party or externally hosted services such as reporting services, security, analytics, monitoring &amp;amp; logging, external API management, big data, and other partner data services. Finally, the iDaaS generic architecture provides both conformance and insights into the knowledge being managed by the offered solutions. For simplicity these elements were left out of this specific architecture diagram. Next up, a look at the architectural solution with a focus on the data view. IDAAS HL7 AND FHIR INTEGRATION (DATA) Data connectivity through the iDaaS connect HL7 and FHIR integration architecture provides a different look at the architecture and gives us insights into how one of the most valuable assets of a healthcare organisation is being processed. It should be seen as the architecture is intended, as a guide and not a definitive must-do-it-this-way statement on how the data is being routed through as actors are engaging with the systems, applications, and services in this architecture. Note that many of the data flows only one direction while it's fairly obvious it's going to flow both ways. We've chosen to note that in the flows that do not disrupt the clarity of the architecture diagram, and chose not to indicate where the intent is to show processing and flows for clarity from actors to systems on the backend. It's left to the reader to explore these data diagrams and feel free to send comments our way. WHAT'S NEXT This was just a short overview of an example HL7 and FHIR integration architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. 3. 4. 5. Example data insights Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at a more specific architectural example of iDaaS data insights that can be mapped to your own intelligent data as a service solutions.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Keycloak 16.1.1 released</title><link rel="alternate" href="https://www.keycloak.org/2022/01/keycloak-1611-released" /><author><name /></author><id>https://www.keycloak.org/2022/01/keycloak-1611-released</id><updated>2022-01-25T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 16.0 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES ENHANCEMENTS * Upgrade to WildFly 26.0.1.Final keycloak UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title>Create entitled builds for Red Hat subscriptions in OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/24/create-entitled-builds-red-hat-subscriptions-openshift" /><author><name>Mikel Sanchez</name></author><id>bea84f86-ab13-4bdf-b814-c840bdef0201</id><updated>2022-01-24T07:00:00Z</updated><published>2022-01-24T07:00:00Z</published><summary type="html">&lt;p&gt;Entitlements &lt;a href="https://access.redhat.com/solutions/3653861"&gt;add security to deployments&lt;/a&gt; in Red Hat services and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. However, entitled builds require extra work. This article details the process step-by-step. We will start with getting the entitlements, certificate authority (CA), and configuration files, and end with a &lt;code&gt;BuildConfig&lt;/code&gt; in OpenShift.&lt;/p&gt; &lt;p&gt;Recently, I was engaged in a project where I had to create a Jenkins Maven agent with Selenium Server and Google Chrome. I realized that to install the Chrome client, I needed several repositories on &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; or &lt;a href="https://www.centos.org"&gt;CentOS&lt;/a&gt;. I followed the official OpenShift documentation and did other research to achieve the deployment. This article shows my process.&lt;/p&gt; &lt;h2&gt;Subscription entitlements&lt;/h2&gt; &lt;p&gt;Subscriptions and systems are managed globally through the Red Hat entitlement service, which is integrated with the Customer Portal. Whenever a subscription is attached to a system, the system is entitled to support services and content for that product. The subscription certificates attached to a system are in a certificate file located in the &lt;code&gt;/etc/pki/entitlement/&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;Entitlement certificates contain additional information about available products and configured content repositories.&lt;br /&gt; A system is allocated a product subscription by subscribing to the entitlement pool that makes that product available, and unsubscribing a machine removes it from the product or entitlement pool, which releases that entitlement subscription it had consumed.&lt;/p&gt; &lt;h2&gt;Getting the entitlements, CA, and configuration&lt;/h2&gt; &lt;p&gt;There are two ways to get the entitlements: through the &lt;a href="https://access.redhat.com/management"&gt;Red Hat Subscription Management console&lt;/a&gt; or from an image. This article covers the second method.&lt;/p&gt; &lt;p&gt;Download a Universal Base Image (UBI) from the &lt;a href="https://access.redhat.com/containers"&gt;Red Hat Ecosystem Catalog&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman pull registry.access.redhat.com/ubi8/ubi:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create a new &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --name ubi8 registry.access.redhat.com/ubi8/ubi:latest tail -f /dev/null&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute a new terminal inside the container and subscribe it with your credentials:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman exec -ti ubi8 /bin/bash $ subscription-manager register --username=&lt;USERNAME&gt; --password=&lt;PASSWORD&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Get the certificates from the container file system and save them on your computer:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman cp ubi8:/etc/rhsm rhsm -&gt; conf + ca $ podman cp ubi8:/etc/pki/entitlement -&gt; entitlement file&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From the files copied, the ones that you need are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Entitlements: &lt;code&gt;/etc/pki/entitlement&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt; -&gt; XXXXX.pem -&gt; XXXXX-key.pem &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Configuration: &lt;code&gt;/etc/rhsm/&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt; -&gt; rhsm.conf &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;CA: &lt;code&gt;/etc/rhsm/ca&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt; -&gt; redhat-entitlement-authority.pem -&gt; redhat-uep.pem&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Creating the Dockerfile&lt;/h2&gt; &lt;p&gt;I used the following Dockerfile to cover all the requirements from the client. The Dockerfile contains some comments to help you understand it. All the files except the entitlements can be retrieved from my &lt;a href="https://github.com/misanche/jenkins-slave-selenium/tree/ocp"&gt;GitHub repository&lt;/a&gt;. The file is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM registry.redhat.io/openshift4/ose-jenkins-agent-maven:v4.9 USER root # Copy entitlements COPY ./etc-pki-entitlement /etc/pki/entitlement # Copy subscription manager configurations COPY ./rhsm-conf /etc/rhsm COPY ./rhsm-ca /etc/rhsm/ca # Add Chrome Repo COPY google-chrome.repo /etc/yum.repos.d/google-chrome.repo # Add chrome-driver installation script COPY install-chrome-driver.sh . # Copy Selenium server COPY selenium-server-standalone-3.141.59.jar . # Copy Init Script COPY init.sh . # Delete /etc/rhsm-host to use entitlements from the build container RUN rm /etc/rhsm-host &amp;&amp; \ # Initialize /etc/yum.repos.d/redhat.repo # See https://access.redhat.com/solutions/1443553 yum repolist --disablerepo=* &amp;&amp; \ subscription-manager repos --enable rhel-8-for-x86_64-appstream-rpms &amp;&amp; \ yum -y update &amp;&amp; \ yum -y install google-chrome-stable &amp;&amp; \ # Remove entitlements and Subscription Manager configs rm -rf /etc/pki/entitlement &amp;&amp; \ rm -rf /etc/rhsm # Install Chrome &amp; chromedriver RUN ./install-chrome-driver.sh USER 1001 ENTRYPOINT [ "sh", "./init.sh" ]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deployment on OpenShift&lt;/h2&gt; &lt;p&gt;This section covers the commands you need to run the final &lt;code&gt;BuildConfig&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Secrets&lt;/h3&gt; &lt;p&gt;This PKI entitlements secret will be injected into the container when the &lt;code&gt;BuildConfig&lt;/code&gt; runs. Create the secret as follows :&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic etc-pki-entitlement \ --from-file &lt;location&gt;/&lt;-key.pem&gt;.pem \ --from-file &lt;location&gt;/&lt;number&gt;-key.pem&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the configuration secret:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic rhsm-conf --from-file &lt;location&gt;/rhsm.conf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, create the secret with the CA:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic rhsm-ca \ --from-file &lt;location&gt;/redhat-entitlement-authority.pem \ --from-file &lt;location&gt;/redhat-uep.pem&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;BuildConfig&lt;/h3&gt; &lt;p&gt;Now we'll create the &lt;code&gt;BuildConfig&lt;/code&gt; with the right build secrets mapped to the desired location. If you are using the &lt;a href="https://github.com/openshift/jenkins"&gt;Jenkins Source-to-Image (S2I) image&lt;/a&gt;, you can also add the label that appears in the following command to allow the &lt;a href="https://github.com/openshift/jenkins-sync-plugin"&gt;OpenShift Sync plugin&lt;/a&gt; to automatically create the slave:&lt;/p&gt; &lt;pre lang="bash" xml:lang="bash"&gt; &lt;code&gt;$ oc new-build https://github.com/misanche/jenkins-slave-selenium.git#ocp \ --name=jenkins-slave-chrome1 -e GIT_SSL_NO_VERIFY=true \ --build-secret "etc-pki-entitlement:etc-pki-entitlement" \ --build-secret "rhsm-conf:rhsm-conf" --build-secret "rhsm-ca:rhsm-ca" \ --labels=role=jenkins-slave -n &lt;namespace&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These steps should allow you to create entitled builds in OpenShift.&lt;/p&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;To summarize, the process can be hard at first, but it is easier to do it once you know the concepts. Here are a few other useful resources to explore:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/how-to-use-entitled-image-builds-to-build-drivercontainers-with-ubi-on-openshift"&gt;How to use entitled image builds to build DriverContainers with UBI on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://examples.openshift.pub/build/entitled/"&gt;Entitled builds on OpenShift 4&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/24/create-entitled-builds-red-hat-subscriptions-openshift" title="Create entitled builds for Red Hat subscriptions in OpenShift"&gt;Create entitled builds for Red Hat subscriptions in OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mikel Sanchez</dc:creator><dc:date>2022-01-24T07:00:00Z</dc:date></entry><entry><title type="html">Migrating from Spring Boot to Quarkus with MTA</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/migrating-from-spring-boot-to-quarkus-with-mta/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=migrating-from-spring-boot-to-quarkus-with-mta" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/migrating-from-spring-boot-to-quarkus-with-mta/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=migrating-from-spring-boot-to-quarkus-with-mta</id><updated>2022-01-21T17:48:00Z</updated><content type="html">In this article we will walk through a sample migration of a Spring Boot REST Application to Quarkus using Red Hat Migration Toolkit for Applications (MTA). Overview of Red Hat Migration Toolkit The Migration Toolkit for Applications (MTA) is an extensible tool which you can use to simplify the migration of several Java applications. This ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Querying Infinispan with the new .NET core client</title><link rel="alternate" href="https://infinispan.org/blog/2022/01/21/dotnet-core-query" /><author><name>Vittorio Rigamonti</name></author><id>https://infinispan.org/blog/2022/01/21/dotnet-core-query</id><updated>2022-01-21T12:00:00Z</updated><content type="html">Dear Infinispanners and .NET Core lovers, Time has passed, keys have been pressed and features have been implemented since we , some months ago, the new .NET core client project. To show you our progress with the project we’ve decided to write a demo/tutorial on how you can run remote queries against Infinispan Server deployments in your .NET Core application. The demo aims to prove the overall maturity of the client and highlight some key client features like: * Shipping as a NuGet package, making it easy to include in C# projects. * Using non-blocking core to add native support for concurrency. * Using language-neutral data types with the Protobuf media type for interoperability. Want to know more go to the and try it! We hope this is a good start to 2022 for all Infinispan C# developers! Please let us know your thoughts. A good place to share them is the page. The Infinispan Team</content><dc:creator>Vittorio Rigamonti</dc:creator></entry></feed>
